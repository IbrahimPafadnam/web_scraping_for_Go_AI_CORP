{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lbHtSCA0iPdl"
      },
      "source": [
        "# **Web Scraping**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "e7-vQYVSiM3P"
      },
      "outputs": [],
      "source": [
        "from bs4 import BeautifulSoup\n",
        "from bs4.element import Tag\n",
        "import requests\n",
        "import pandas as pd\n",
        "import glob\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "OsbqciJHIrR4"
      },
      "outputs": [],
      "source": [
        "url = \"https://www.webonary.org/moore/browse/browse-vernacular-english/?key=mos&letter=a&lang=en\"\n",
        "html_code = requests.get(url).content\n",
        "\n",
        "source = BeautifulSoup(html_code, \"html.parser\")\n",
        "\n",
        "\n",
        "page_info_element = source.find(\"li\", class_=\"page_info\")\n",
        "if page_info_element:\n",
        "    pages = int(page_info_element.text.split(\" \")[3])\n",
        "else:\n",
        "    print(\"Pas de pagination.\")\n",
        "    pages = 1\n",
        "\n",
        "urls = []\n",
        "\n",
        "def extract_url(pages) -> dict:\n",
        "    for i in range(1, pages + 1):\n",
        "        url_pages = f\"https://www.webonary.org/moore/browse/browse-vernacular-english/?key=mos&letter=a&lang=en&pagenr={i}\"\n",
        "        urls.append(url_pages)\n",
        "    return urls\n",
        "\n",
        "urls = extract_url(pages)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "BvF9dGnvNiyi"
      },
      "outputs": [],
      "source": [
        "def extract_data(article: Tag) -> dict:\n",
        "    translation_span = article.find(\"span\", class_=\"translation\")\n",
        "    examples_span = article.find(\"span\", class_=\"example\")\n",
        "\n",
        "\n",
        "    fr = translation_span.find(\"span\", lang='fr').text if translation_span and translation_span.find(\"span\", lang='fr') else None\n",
        "\n",
        "\n",
        "    mos = examples_span.find(\"span\", lang='mos').text if examples_span and examples_span.find(\"span\", lang='mos') else None\n",
        "\n",
        "    return {\"mos\": mos, \"fr\": fr}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "LhzQG-Uji5Ib"
      },
      "outputs": [],
      "source": [
        "def data():\n",
        "    all_data = []\n",
        "    for i in urls:\n",
        "        html_code = requests.get(i).content\n",
        "        sources = BeautifulSoup(html_code, \"html.parser\")\n",
        "        fr_mos = sources.find_all(\"span\", class_=\"post\")\n",
        "        data = [extract_data(article) for article in fr_mos]\n",
        "        all_data.extend(data)\n",
        "    return all_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "P-yP6SolsHSe"
      },
      "outputs": [],
      "source": [
        "data = data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "r1IKnH8cxGiS"
      },
      "outputs": [],
      "source": [
        "# Enregistrement sous fichier csv\n",
        "df = pd.DataFrame.from_dict(data)\n",
        "df.to_csv(\"data_a.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ojJs0XYxm2p"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1_z1EuWMVvbJ"
      },
      "source": [
        "# Collecte et netoyage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "DhZv63LqZlWC"
      },
      "outputs": [],
      "source": [
        "# Spécifiez le chemin du répertoire contenant vos fichiers CSV\n",
        "chemin_repertoire = \"/content/\"  # Remplacez par le chemin réel\n",
        "\n",
        "# Obtenez une liste de tous les fichiers CSV dans le répertoire\n",
        "tous_les_fichiers = glob.glob(os.path.join(chemin_repertoire, \"*.csv\"))\n",
        "\n",
        "# Créez une liste pour stocker les DataFrames individuels\n",
        "liste_df = []\n",
        "\n",
        "# Parcourez chaque fichier CSV et lisez-le dans un DataFrame\n",
        "for fichier in tous_les_fichiers:\n",
        "    df = pd.read_csv(fichier)\n",
        "    liste_df.append(df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "h2lDh34g8zay"
      },
      "outputs": [],
      "source": [
        "# Concaténez tous les DataFrames en un seul DataFrame\n",
        "data_final = pd.concat(liste_df, ignore_index=True)\n",
        "\n",
        "# Affichez le DataFrame combiné\n",
        "data_final.dropna(inplace=True)\n",
        "data_final.reset_index(drop=True, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "sGOEzZ5h97mH"
      },
      "outputs": [],
      "source": [
        "data_final.to_csv(\"data_final.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "FzI7ntAebXjT"
      },
      "outputs": [],
      "source": [
        "# Export sous format json\n",
        "import json\n",
        "df = pd.read_csv(\"data_final.csv\")\n",
        "\n",
        "data_json = []\n",
        "for _, row in df.iterrows():\n",
        "    data_json.append({\n",
        "        \"translation\": {\n",
        "            \"fr\": row[\"fr\"],\n",
        "            \"mos\": row[\"mos\"]\n",
        "        }\n",
        "    })\n",
        "\n",
        "with open(\"datas.json\", \"w\", encoding=\"utf-8\") as json_file:\n",
        "    json.dump(data_json, json_file, ensure_ascii=False, indent=3)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
